{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea234a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "# from unittest import result\n",
    "from hazm import *\n",
    "from news import NewsDocument\n",
    "\n",
    "from unittest import result\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a123ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "garbage = ['۱', '۲', '۳', '۴', '۵', '۶', '۷', '۸', '۹', '۰', 'a', 'b', 'c', 'd', 'e', 't', 'o', 'p', 'x', 'y', 'z',\n",
    "           'https', '،', '.', ':', '**', '-', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '?', '**', '[', ']',\n",
    "           '(', ')', '://', '/?', '=', '&', '/', '؛', '&', '/', '.', '_', '،', '?**', \":\", \"%\", \">>\", \"<<\", \"!\",\"#\"\n",
    "           \"*\", \"«\", \"»\"]\n",
    "\n",
    "normalizer = Normalizer()\n",
    "lemmatizer = Lemmatizer()\n",
    "stemmer = Stemmer()\n",
    "stop_words_list = stopwords_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80182dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_dictionary_and_vectors():\n",
    "\n",
    "  # f = open('data/sample.json', encoding='utf-8')\n",
    "  f = open('data/IR_data_news_12k.json', encoding='utf-8')\n",
    "  all_documents = json.load(f)\n",
    "  all_docs_count = len(all_documents)\n",
    "  print(all_docs_count)\n",
    "  all_news = []\n",
    "\n",
    "  words_dictionary = {} # term - document frequency\n",
    "  term_postings = {} # term - postings list\n",
    "  # previous_doc_ID = -1\n",
    "\n",
    "  for doc_ID in all_documents:\n",
    "  \n",
    "    print(doc_ID) \n",
    "    initial_tokens = word_tokenize(normalizer.normalize(all_documents[doc_ID][\"content\"]))\n",
    "    tokens = preprocess_tokens(initial_tokens)\n",
    "    doc_object = NewsDocument(doc_ID, tokens, all_docs_count, all_documents[doc_ID][\"url\"])\n",
    "    all_news.append(doc_object)\n",
    "    \n",
    "    for word in tokens:\n",
    "      if word in words_dictionary:\n",
    "\n",
    "        \n",
    "        term_postings[word].add(doc_object)\n",
    "        words_dictionary[word] = len(term_postings[word])\n",
    "\n",
    "      else:\n",
    "      \n",
    "       term_postings[word] = set()\n",
    "       term_postings[word].add(doc_object)\n",
    "       words_dictionary[word] = len(term_postings[word])\n",
    "      #  previous_doc_ID = doc_ID\n",
    "\n",
    "    # print(words_dictionary)\n",
    "  i = 0\n",
    "  for news in all_news:\n",
    "    print(i)\n",
    "    news.create_vector(words_dictionary)\n",
    "    i+= 1\n",
    "\n",
    "\n",
    "  #saving data\n",
    "\n",
    "  file = open(\"words_dictionary.pkl\",\"wb\")\n",
    "  pickle.dump(words_dictionary, file) \n",
    "  file.close()\n",
    "  file = open(\"all_news.pkl\",\"wb\")\n",
    "  pickle.dump(all_news, file) \n",
    "  file.close()\n",
    "  file = open(\"term_postings.pkl\",\"wb\")\n",
    "  pickle.dump(term_postings, file) \n",
    "  file.close()\n",
    "\n",
    "  return words_dictionary, all_news, term_postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34416d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_files():\n",
    "\n",
    "  #oppening data files \n",
    "  print(\"getting dictionary ......\")\n",
    "  file = open(\"pickleFiles/words_dictionary.pkl\", \"rb\")\n",
    "  words_dictionary = pickle.load(file)\n",
    "  file.close()\n",
    "  print(\"getting documents ......\")\n",
    "  file = open(\"pickleFiles/all_news.pkl\", \"rb\")\n",
    "  all_news = pickle.load(file)\n",
    "  file.close()\n",
    "  print(\"getting champion list ......\\n\")\n",
    "  file = open(\"pickleFiles/champion_list.pkl\", \"rb\")\n",
    "  term_champion_list = pickle.load(file)\n",
    "  file.close()\n",
    "  print(\"getting term postings ......\\n\")\n",
    "  file = open(\"pickleFiles/term_postings.pkl\", \"rb\")\n",
    "  term_postings = pickle.load(file)\n",
    "  file.close()\n",
    "  print(\"Done \\n\")\n",
    "\n",
    "  return words_dictionary, all_news, term_champion_list, term_postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c8da216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_query(input_query, count, dictionary, all_news, term_postings):\n",
    "  tokens = preprocess_tokens(word_tokenize(normalizer.normalize(input_query)))\n",
    "  query_object = NewsDocument(-1, tokens, len(all_news), None)  # should change N ??????????????????\n",
    "  query_object.create_vector(dictionary)\n",
    "  query_object.find_cosine_distances_from_all_news(all_news, term_postings)\n",
    "  # count = input(\"Enter number of k for showing top k results\\n\")\n",
    "  top_news,cosines,urls = query_object.get_top_nearest_news(count=int(count))\n",
    "  # print(cosines)\n",
    "  return top_news,cosines,urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9771fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_champions_list(count, words_dictionary, all_news, term_postings):\n",
    "  term_champion_list = {}\n",
    "  print(\"creating champions list ...\")\n",
    "  i=0\n",
    "  for term in list(words_dictionary):\n",
    "    top_news, _ , urls = search_query(term, count, words_dictionary, all_news, term_postings)\n",
    "    term_champion_list[term] = top_news\n",
    "    print(term, i)\n",
    "    i += 1\n",
    "  \n",
    "  file = open(\"champion_list.pkl\",\"wb\")\n",
    "  pickle.dump(term_champion_list, file) \n",
    "  file.close()\n",
    "  print(\"Done \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe267c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_using_champion_list(input_query, words_dictionary, all_news, term_champion_list):\n",
    "\n",
    "  # print(\"getting champion list ......\\n\")\n",
    "  # file = open(\"pickleFiles/champion_list.pkl\", \"rb\")\n",
    "  # term_champion_list = pickle.load(file)\n",
    "  # file.close()\n",
    "  related_news = []\n",
    "  \n",
    "  query_tokens = preprocess_tokens(word_tokenize(normalizer.normalize(input_query)))\n",
    "  \n",
    "  for token in query_tokens:\n",
    "    related_news.extend(term_champion_list[token])\n",
    "\n",
    "\n",
    "  query_object = NewsDocument(-1, query_tokens, len(all_news), None)  # should change N ??????????????????\n",
    "  query_object.create_vector(words_dictionary)\n",
    "  query_object.find_cosine_distances_from_related_news(related_news)\n",
    "  count = input(\"Enter number of k for showing top k results\\n\")\n",
    "  top_news,cosines,urls = query_object.get_top_nearest_news(count=int(count))\n",
    "  \n",
    "  return top_news,cosines,urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1f5da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tokens(tokens):\n",
    "\n",
    "  tokens_without_stop_words = []\n",
    "\n",
    "  for token in tokens:\n",
    "    token = re.sub(r'[^\\w\\s]','', token)\n",
    "    if token not in stop_words_list:\n",
    "      tokens_without_stop_words.append(token)\n",
    "   \n",
    "  for token in tokens_without_stop_words:\n",
    "    for s in garbage:\n",
    "      if s in token:\n",
    "        # print(\"token garbage\", token)\n",
    "        tokens_without_stop_words.remove(token)\n",
    "        break\n",
    "\n",
    "  pure_root_tokens = list(map(lambda word: lemmatizer.lemmatize(stemmer.stem(word)), tokens_without_stop_words))\n",
    " \n",
    "  return pure_root_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7d64165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dictionary ......\n",
      "getting documents ......\n",
      "getting champion list ......\n",
      "\n",
      "getting term postings ......\n",
      "\n",
      "Done \n",
      "\n",
      "38663\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "  words_dictionary, all_news, term_champion_list, term_postings = get_data_from_files()\n",
    "  # words_dictionary, all_news = get_data_from_files()\n",
    "  # words_dictionary, all_news, term_postings = creating_dictionary_and_vectors()\n",
    "  print(len(words_dictionary))\n",
    "  # create_champions_list(20, words_dictionary, all_news, term_postings)\n",
    "\n",
    "  # term_champion_list={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc0dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your query\n",
      "مردم سالاری\n",
      "1.normal 2.champion\n",
      "1\n",
      "Enter number of k for showing top k results\n",
      "5\n",
      "docID: 10011  cosine: 0.2384275412943792  url: https://www.farsnews.ir/news/14000918000589/نایب-رئیس-مجلس-رئیس‌جمهور-بخشنامه-تعیین-سقف-برای-به‌کارگیری-افراد\n",
      "docID: 9078  cosine: 0.2292957778923553  url: https://www.farsnews.ir/news/14001013000454/شهید-سلیمانی-تراز-جدیدی-از-مسئولیت‌پذیری-را-تعریف-کرد\n",
      "docID: 7446  cosine: 0.22825118381975532  url: https://www.farsnews.ir/news/14001205000409/کواکبیان-برای-لغو-تحریم‌ها-باید-راستی‌آزمایی-و-تضمین-داشته-باشیم\n",
      "docID: 3842  cosine: 0.20402401410759563  url: https://www.farsnews.ir/news/14001103000905/مدیر-عامل-شاهین-سوپرمن-نیستم-دخالت-در-مدیریت-باشگاه-زیاد-است\n",
      "docID: 10900  cosine: 0.20331624898689254  url: https://www.farsnews.ir/news/14000826000357/نظام-آموزشی-ما-نقادی-ذهن-را-پرورش-نداده\n"
     ]
    }
   ],
   "source": [
    "  #getting query\n",
    "  \n",
    "  # for doc in term_postings['پیکان']:\n",
    "  #   print(doc.get_doc_ID())\n",
    "\n",
    "  while(True):\n",
    "    input_query = input(\"Please enter your query\\n\")\n",
    "    if input_query == 'exit':\n",
    "      break\n",
    "    method = input(\"1.normal 2.champion\\n\")\n",
    "    top_news, cosines, urls = None, None, None\n",
    "\n",
    "    if int(method) == 1:\n",
    "      count = input(\"Enter number of k for showing top k results\\n\")\n",
    "      top_news,cosines,urls = search_query(input_query, count, words_dictionary, all_news, term_postings)\n",
    "\n",
    "      # query_object = NewsDocument(-1, preprocess_tokens(word_tokenize(normalizer.normalize(input_query))), len(all_news), None)  # should change N ??????????????????\n",
    "      # query_object.create_vector(words_dictionary)\n",
    "      # query_object.find_cosine_distances_from_all_news(all_news)\n",
    "      # count = input(\"Enter number of k for showing top k results\\n\")\n",
    "      # top_news,cosines,urls = query_object.get_top_nearest_news(count=int(count))\n",
    "    elif int(method) == 2:\n",
    "      top_news,cosines,urls = search_using_champion_list(input_query, words_dictionary, all_news, term_champion_list)\n",
    "      # continue\n",
    "    i=0\n",
    "    for news in top_news:\n",
    "      print(\"docID: {}  cosine: {}  url: {}\".format(news.get_doc_ID(), cosines[i], urls[i]))\n",
    "      i+=1\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb335d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
